# -*- coding: utf-8 -*-
"""2_AICE_Professional_문제풀이_특강_Text_정답_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iBixMXs5paVI5SoV0Zbi6nIJ2tPW-cdv

+ keras 토큰나이져를 활용하여 토큰화하고 일정 크기의 문장으로 맞추기 위해 패딩 및 embedding 레이어를 이용해서 단어를 n차원 밀집벡터를 만들고 train시에 학습되도록 합니다.그리고 나서 LSTM 모델을 이용하여 감성분류를 수행합니다.

<br>
<br>

#   
## 1. 라이브러리 임포트
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import urllib.request

"""#   
## 2. 파일 읽어오기
"""

# 네이버 영화 리뷰 데이터 읽어오기
# urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
# urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")

!git clone https://github.com/e9t/nsmc.git

# 데이터 확인하기
final_data = pd.read_csv("./nsmc/ratings.txt", delimiter='\t', quoting=3)

final_data.head(3)

# 총 200000 건
final_data.info()

"""#   
## 3. 전처리 : Null, 중복 제거
"""

# final_data 어떤 컬럼과 내용으로 되어 있는지 다시 확인
final_data.tail()

# '문장' 컬럼의 내용을 양끝의 빈공간 삭제
final_data['document'] = final_data['document'].str.strip()

# Null 있는지 확인 : 없음
final_data.isnull().sum()

# null 데이터 삭제
final_data.dropna(inplace=True)

# 중복 데이터 있는지 확인 : 중복 존재 확인
final_data['document'].duplicated().sum()

# 중복 데이터 제거
final_data.drop_duplicates(subset=['document'], inplace=True)

# info 정보 확인
final_data.info()

"""#   
## 4. 영문, 숫자, 특수문자 제거
"""

# '문장' 컬럼의 내용중에 영문, 특수문자 있는지 확인 : 영문과 특수문자 존재 확인
final_data[ final_data['document'].str.contains('[^가-힣 ]') ].head(3)

# '문장' 컬럼의 내용에서 숫자, 영문자, 특수문자등의 글자는 삭제처리
# final_data['문장'].replace('[^가-힣 ]','', regex=True) : 이렇게도 가능

final_data['document'] = final_data['document'].str.replace('[^가-힣 ]','', regex=True)

# '문장' 컬럼의 내용에서 영문, 특수문자 없음 확인
final_data['document'][final_data['document'].str.contains('[^가-힣 ]')].sum()

# 숫자, 영문자, 특수문자 등 제거후 데이터 확인하기.
final_data.head(3)

"""#   
## 5. Label 분포 확인
"""

# label '감정' 분포 확인 : 총 6개이며, 고루게 분포 확인. 단 기쁨이 약간 부족해 보임
final_data['label'].value_counts()

"""#   
## 6. X, Y 분리
"""

# X, Y 분리
features = final_data['document'].values
labels = final_data['label'].values

features.shape, labels.shape

# features 내용 3개 출력
features[:3]

print('이벤트 문자열 최대 길이 :{}'.format(max(len(l) for l in features)))
print('이벤트 문자열 평균 길이 :{}'.format(sum(map(len, features))/len(features)))

# 히스토그램을 보면 30~40 부근에 많이 몰려 있음 알수 있다.
plt.hist([len(s) for s in features], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

"""#   
## 7. train set와 test set 분리
"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(features, labels , test_size=0.2, stratify=labels, random_state=41)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

# 샘플확인 , 라벨 확인
# {0: '불안', 1: '분노', 2: '상처', 3: '슬픔', 4: '당황', 5: '기쁨'}

x_train[:2], y_train[:2]

"""#   
## 8. 전체 문장에 대해 Tokenizing
+ 컴퓨터가 이해하기 위해 모든 단어를 숫자로 변환해야 함.
+ 단어 빈도수 따지지 않고 무조건 모든 단어 수용해서 진행
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenizer 구현 : 단어 사전 만들기(fit_on_texts)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)

# 단어에 대한 숫자 매핑
print(tokenizer.word_index)

# 반대로 숫자로 단어 매핑
print(tokenizer.index_word)

# 단어별 빈도수 확인
print(tokenizer.word_counts)

# 총 단어 갯수 : 47,646
max_words = len(tokenizer.index_word)
print(max_words)

"""#   
## 9. texts_to_sequences : 문장을 숫자로 나열
- 빈도수 적은 단어 제외하는것 없이 모든 단어 포함해서 진행
- 그리고, 예를 들어 1번 등장하는 단어는 삭제하는 작업은 필요시 수행!!
"""

# 문장을 숫자로 나열
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

# 문장을 숫자로 변경후 갯수 확인
# x_train.shape, x_test.shape, y_train.shape, y_test.shape : ((41259,), (10315,), (41259,), (10315,))
print(len(x_train_seq), len(x_test_seq))

print(x_train[1:3])
print(x_train_seq[1:3])

"""#   
## 10. Padding Sequence
"""

# 문장의 최대 길이 파악 : 제일 긴 문장 seq 길이는 38개로 구성됨.
max(len(line) for line in x_train_seq)

# 모든 문장을 최대 문장 Seq 길이 42에 맞춘다.
x_train_pad = pad_sequences(x_train_seq, maxlen=42)
x_test_pad = pad_sequences(x_test_seq, maxlen=42)

# 문장 Seq 내용을 보니 잘 패딩되어 있음 확인
x_train_pad[:1]

# 문장 Seq 패딩의 shape 확인
x_train_pad.shape, x_test_pad.shape

"""#   
## 11. LSTM 모델링
"""

from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPool2D
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, SimpleRNN, GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# 하이퍼 파라미터

max_words = 47646 + 1    # 총 단어 갯수 + padding 0 번호
max_len = 42             # 최대 문장 길이
embedding_dim = 64      # embedding 차원

# 모델 선언
model = Sequential()

# 단어를 의미있는 64 차원으로 Vector 변경(Embedding)
model.add(Embedding(max_words, embedding_dim, input_length=max_len))

# LSTM 모델
model.add(LSTM(16, return_sequences=True))
model.add(LSTM(16, return_sequences=True))
model.add(Flatten())
model.add(Dense(128, activation='swish'))
model.add(Dense(32, activation='swish'))
model.add(Dense(2, activation='softmax'))

model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

# 조기종료 콜백함수 정의(EarlyStopping)
es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)

# 체크포인트 저장(EarlyStopping)
checkpoint_path = 'tmp_checkpoint.keras'
cp = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 학습(fit) : 77% 성능 보임
history = model.fit(x_train_pad, y_train, epochs=50, batch_size=512,
                      validation_data=(x_test_pad, y_test), verbose =1, callbacks=[es, cp])

epochs = range(1, len(history.history['accuracy']) + 1)
plt.plot(epochs, history.history['accuracy'])
plt.plot(epochs, history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], )
plt.show()

model.evaluate(x_test_pad, y_test)

"""#   
## 12. 예측해 보기
"""

print(f'문자열 : {x_test[0]}')
print(f'Sequence : {x_test_pad[0]}')

# 모델 예측하기(predict)
predict = model.predict(x_test_pad[:1])

predict

"""

+ BERT 이용하여 Classification하기
+ KoBert 활용하여 감정말뭉치 데이터 감성 분류


#   
## 1. 라이브러리 임포트
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import urllib.request

"""#   
## 2. 파일 읽어오기
"""

# 네이버 영화 리뷰 데이터 읽어오기
# urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
# urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")

!git clone https://github.com/e9t/nsmc.git

# 데이터 확인하기
final_data = pd.read_csv("./nsmc/ratings.txt", delimiter='\t', quoting=3)

final_data.head(3)

# 총 200000 건
final_data.info()

"""#   
## 3. 전처리 : Null, 중복 제거
"""

# final_data 어떤 컬럼과 내용으로 되어 있는지 다시 확인
final_data.tail()

# '문장' 컬럼의 내용을 양끝의 빈공간 삭제
final_data['document'] = final_data['document'].str.strip()

# Null 있는지 확인 : 없음
final_data.isnull().sum()

# null 데이터 삭제
final_data.dropna(inplace=True)

# 중복 데이터 있는지 확인 : 중복 존재 확인
final_data['document'].duplicated().sum()

# 중복 데이터 제거
final_data.drop_duplicates(subset=['document'], inplace=True)

# info 정보 확인
final_data.info()

"""#    
## 4. Okt 라이브러리를 활용한 토큰화
"""

# 일부 데이터만 불러오기
# Colab의 경우에도 해당 데이터를 처리하기 어렵고 오래 걸리기 때문에 임의로 2만 개의 데이터만 설정합니다.

final_data = pd.concat([final_data.iloc[:10000], final_data.iloc[-10000:]])

final_data.shape

# 2만건 데이터를 okt 토크나이즈 하는데 2~3분 소요됨.
# okt.morphs() : 텍스트를 형태소 단위 나누고, stem=True : 각 단어에서 어간을 추출

!pip install konlpy

from konlpy.tag import Okt
okt = Okt()
final_data['document'] = final_data['document'].map(lambda x: ' '.join(okt.morphs(x, stem = True)))

final_data.head(3)

"""#   
## 5. 훈련-테스트 데이터 분할

"""

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(final_data, test_size=0.2, random_state=42)

train_texts = train_df['document'].astype(str).tolist() # 문자열 데이터로 명시 후 리스트 화
train_labels = train_df['label'].tolist()
test_texts = test_df['document'].astype(str).tolist()
test_labels = test_df['label'].tolist()

"""#   
## 6. BERT 토크나이저 불러오기
"""

# !pip install transformers

from transformers import BertTokenizer, BertForSequenceClassification

model_name = 'monologg/kobert'
tokenizer = BertTokenizer.from_pretrained(model_name)

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

"""#   
## 7. DataLoader 구성
"""

# PyTorch에서 제공하는 DataLoader를 활용하여 훈련 데이터를 섞고(Shuffle) 배치(Batch) 단위로 불러올 수 있습니다.

import torch
from torch.utils.data import DataLoader, Dataset

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomDataset(train_encodings, train_labels)
test_dataset = CustomDataset(test_encodings, test_labels)
batch_size = 64  # 배치 사이즈는 직접 지정해야 합니다.
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""#   
## 8.BERT 모델 불러오기
"""

# 0, 1로 분류하기 때문에 레이블은 2개로 지정합니다.
model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)

"""#   
## 9. 모델 훈련
"""

# 1 epochs 당 5분정도 소요됨. 5 epochs 이면 30분정도 시간 걸림.

from tqdm.auto import tqdm
import time

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPU 사용이 가능한 경우 설정

start = time.time()

num_epochs = 5
learning_rate = 2e-5 #2e-5는 0.00002
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()
model.to(device) # GPU 사용이 가능한 경우

for epoch in range(num_epochs):
    model.train() # 훈련 모드 지정
    total_loss = 0

    for batch in tqdm(train_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}")


print(f'총학습시간: { end - start }')

"""#   
## 10. 모델 테스트
"""

# 71% 성능 보임

model.eval()
correct_predictions = 0
total_predictions = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        _, predicted_labels = torch.max(outputs.logits, dim=1)

        correct_predictions += torch.sum(predicted_labels == labels).item()
        total_predictions += labels.size(0)

accuracy = correct_predictions / total_predictions
print(f"Test Accuracy: {accuracy:.4f}")

"""#   
### 11. 추론

"""

input_text = '이 영화 진짜 재밌다'
input_encoding = tokenizer.encode_plus(
    input_text,
    truncation=True,
    padding=True,
    return_tensors='pt'
)

input_ids = input_encoding['input_ids'].to(device)
attention_mask = input_encoding['attention_mask'].to(device)

model.eval()
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask)
    _, predicted_labels = torch.max(outputs.logits, dim=1)
predicted_labels = predicted_labels.item()

print(predicted_labels)
